{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1085454,"sourceType":"datasetVersion","datasetId":605165}],"dockerImageVersionId":29867,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classify emotions in text with BERT NLP model ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.                                    \n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:33.600928Z","iopub.execute_input":"2024-11-28T11:13:33.601312Z","iopub.status.idle":"2024-11-28T11:13:33.609239Z","shell.execute_reply.started":"2024-11-28T11:13:33.601269Z","shell.execute_reply":"2024-11-28T11:13:33.608516Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/emotions-dataset-for-nlp/val.txt\n/kaggle/input/emotions-dataset-for-nlp/test.txt\n/kaggle/input/emotions-dataset-for-nlp/train.txt\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:33.611420Z","iopub.execute_input":"2024-11-28T11:13:33.611731Z","iopub.status.idle":"2024-11-28T11:13:39.944986Z","shell.execute_reply.started":"2024-11-28T11:13:33.611695Z","shell.execute_reply":"2024-11-28T11:13:39.944230Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (2.7.0)\nRequirement already satisfied: tokenizers==0.5.2 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.5.2)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.12.32)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.42.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.85)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.38)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers) (0.7)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.18.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2020.2.20)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.9.5)\nRequirement already satisfied: botocore<1.16.0,>=1.15.32 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (1.15.32)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->transformers) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->transformers) (2.8.1)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io\n#matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:39.946844Z","iopub.execute_input":"2024-11-28T11:13:39.947077Z","iopub.status.idle":"2024-11-28T11:13:40.109897Z","shell.execute_reply.started":"2024-11-28T11:13:39.947053Z","shell.execute_reply":"2024-11-28T11:13:40.109242Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# identify and specify the GPU as the device, later in training loop we will load data into device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 19\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.110959Z","iopub.execute_input":"2024-11-28T11:13:40.111152Z","iopub.status.idle":"2024-11-28T11:13:40.166984Z","shell.execute_reply.started":"2024-11-28T11:13:40.111131Z","shell.execute_reply":"2024-11-28T11:13:40.166225Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.169846Z","iopub.execute_input":"2024-11-28T11:13:40.170107Z","iopub.status.idle":"2024-11-28T11:13:40.177519Z","shell.execute_reply.started":"2024-11-28T11:13:40.170081Z","shell.execute_reply":"2024-11-28T11:13:40.176879Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"BertTokenizer to run end-to-end tokenization: punctuation splitting + word piece. \nBertForSequenceClassification is the Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output). \nBertConfig is the configuration class to store model configurations. \nAdamW implements Adam learning rate optimization algorithm, it is a type of Stochastic Gradient Descent with momentum. Here momentum is described as the moving average of the gradient instead of gradient itself.\nget_linear_schedule_with_warmup creates a schedule with a learning rate that decreases linearly after linearly increasing during a warm-up period.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/emotions-dataset-for-nlp/train.txt\", delimiter=';', header=None, names=['sentence','label'])\ndf_test = pd.read_csv(\"/kaggle/input/emotions-dataset-for-nlp/test.txt\", delimiter=';', header=None, names=['sentence','label'])\ndf_val = pd.read_csv(\"/kaggle/input/emotions-dataset-for-nlp/val.txt\", delimiter=';', header=None, names=['sentence','label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.179986Z","iopub.execute_input":"2024-11-28T11:13:40.180184Z","iopub.status.idle":"2024-11-28T11:13:40.297349Z","shell.execute_reply.started":"2024-11-28T11:13:40.180162Z","shell.execute_reply":"2024-11-28T11:13:40.296768Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df = pd.concat([df_train,df_test,df_val])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.298779Z","iopub.execute_input":"2024-11-28T11:13:40.299061Z","iopub.status.idle":"2024-11-28T11:13:40.305321Z","shell.execute_reply.started":"2024-11-28T11:13:40.299026Z","shell.execute_reply":"2024-11-28T11:13:40.304769Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df['label'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.306607Z","iopub.execute_input":"2024-11-28T11:13:40.306876Z","iopub.status.idle":"2024-11-28T11:13:40.326172Z","shell.execute_reply.started":"2024-11-28T11:13:40.306852Z","shell.execute_reply":"2024-11-28T11:13:40.325278Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array(['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'],\n      dtype=object)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"    from sklearn.preprocessing import LabelEncoder\n    labelencoder = LabelEncoder()\n    df['label_enc'] = labelencoder.fit_transform(df['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.327307Z","iopub.execute_input":"2024-11-28T11:13:40.327572Z","iopub.status.idle":"2024-11-28T11:13:40.336462Z","shell.execute_reply.started":"2024-11-28T11:13:40.327544Z","shell.execute_reply":"2024-11-28T11:13:40.335750Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"df[['label','label_enc']].drop_duplicates(keep='first')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.337451Z","iopub.execute_input":"2024-11-28T11:13:40.337648Z","iopub.status.idle":"2024-11-28T11:13:40.366273Z","shell.execute_reply.started":"2024-11-28T11:13:40.337615Z","shell.execute_reply":"2024-11-28T11:13:40.365483Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"      label  label_enc\n0   sadness          4\n2     anger          0\n3      love          3\n6  surprise          5\n7      fear          1\n8       joy          2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>label_enc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sadness</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>anger</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>love</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>surprise</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fear</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>joy</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df.rename(columns={'label':'label_desc'},inplace=True)\ndf.rename(columns={'label_enc':'label'},inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.367377Z","iopub.execute_input":"2024-11-28T11:13:40.367600Z","iopub.status.idle":"2024-11-28T11:13:40.374418Z","shell.execute_reply.started":"2024-11-28T11:13:40.367576Z","shell.execute_reply":"2024-11-28T11:13:40.373779Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"## create label and sentence list\nsentences = df.sentence.values\n\n#check distribution of data based on labels\nprint(\"Distribution of data based on labels: \",df.label.value_counts())\n\n# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n# In the original paper, the authors used a length of 512.\nMAX_LEN = 256\n\n## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\ninput_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\nlabels = df.label.values\n\nprint(\"Actual sentence before tokenization: \",sentences[2])\nprint(\"Encoded Input from dataset: \",input_ids[2])\n\n## Create attention mask\nattention_masks = []\n## Create a mask of 1 for all input tokens and 0 for all padding tokens\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids]\nprint(attention_masks[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:40.375562Z","iopub.execute_input":"2024-11-28T11:13:40.375800Z","iopub.status.idle":"2024-11-28T11:13:50.961658Z","shell.execute_reply.started":"2024-11-28T11:13:40.375776Z","shell.execute_reply":"2024-11-28T11:13:50.960792Z"}},"outputs":[{"name":"stdout","text":"Distribution of data based on labels:  2    6761\n4    5797\n0    2709\n1    2373\n3    1641\n5     719\nName: label, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecbb56024b9b4cf09e8f1c71a327e58f"}},"metadata":{}},{"name":"stdout","text":"\nActual sentence before tokenization:  im grabbing a minute to post i feel greedy wrong\nEncoded Input from dataset:  [101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Dataset Prep for training","metadata":{}},{"cell_type":"markdown","source":"#### Split into a training set and a test set using a stratified k fold","metadata":{}},{"cell_type":"code","source":"train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\ntrain_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:50.963121Z","iopub.execute_input":"2024-11-28T11:13:50.963413Z","iopub.status.idle":"2024-11-28T11:13:50.988790Z","shell.execute_reply.started":"2024-11-28T11:13:50.963377Z","shell.execute_reply":"2024-11-28T11:13:50.988186Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# convert all our data into torch tensors, required data type for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_inputs,train_masks,train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\nvalidation_sampler = RandomSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:50.990035Z","iopub.execute_input":"2024-11-28T11:13:50.990312Z","iopub.status.idle":"2024-11-28T11:13:51.437968Z","shell.execute_reply.started":"2024-11-28T11:13:50.990278Z","shell.execute_reply":"2024-11-28T11:13:51.437369Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Lets see whats there in traindata set ","metadata":{}},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:51.439017Z","iopub.execute_input":"2024-11-28T11:13:51.439220Z","iopub.status.idle":"2024-11-28T11:13:51.495929Z","shell.execute_reply.started":"2024-11-28T11:13:51.439199Z","shell.execute_reply":"2024-11-28T11:13:51.495116Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(tensor([ 101, 1045, 2123, 1056, 2514, 2061, 9069, 2035, 1996, 2051,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0]),\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0.]),\n tensor(4))"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"type(train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:51.496961Z","iopub.execute_input":"2024-11-28T11:13:51.497146Z","iopub.status.idle":"2024-11-28T11:13:51.501195Z","shell.execute_reply.started":"2024-11-28T11:13:51.497126Z","shell.execute_reply":"2024-11-28T11:13:51.500519Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6).to(device)\n\n# Parameters:\nlr = 2e-5\nadam_epsilon = 1e-8\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\n\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader)*epochs      \n\n### In Transformers, optimizer and schedules are splitted and instantiated like this:                            \noptimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:13:51.502170Z","iopub.execute_input":"2024-11-28T11:13:51.502349Z","iopub.status.idle":"2024-11-28T11:14:13.534580Z","shell.execute_reply.started":"2024-11-28T11:13:51.502329Z","shell.execute_reply":"2024-11-28T11:14:13.533741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf40ef3a6b0646a8b0a0f2f29f41140e"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564fab24c20346f0a66650fa9746c0f6"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from tqdm.notebook import trange  # Correct import for Jupyter notebooks\n\ntrain_loss_set = []\nlearning_rate = [] \n\n# Gradients get accumulated by default\nmodel.zero_grad()\n\n# Use trange from tqdm.notebook for Jupyter notebooks\nfor _ in trange(1, epochs + 1, desc='Epoch'):\n    print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n    batch_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        # Set model to training mode\n        model.train()\n        \n        # Move batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Forward pass\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs[0]\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update optimizer\n        optimizer.step()\n        \n        # Update scheduler\n        scheduler.step()\n\n        # Clear accumulated gradients\n        optimizer.zero_grad()\n        \n        batch_loss += loss.item()\n\n    # Average training loss for the epoch\n    avg_train_loss = batch_loss / len(train_dataloader)\n\n    # Store the current learning rate\n    for param_group in optimizer.param_groups:\n        print(\"\\n\\tCurrent Learning rate: \", param_group['lr'])\n        learning_rate.append(param_group['lr'])\n    \n    train_loss_set.append(avg_train_loss)\n    print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n    \n    # Validation phase\n    model.eval()\n\n    eval_accuracy, eval_mcc_accuracy, nb_eval_steps = 0, 0, 0\n\n    for batch in validation_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        logits = logits[0].to('cpu').numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        pred_flat = np.argmax(logits, axis=1).flatten()\n        labels_flat = label_ids.flatten()\n        \n        # DataFrame for metrics\n        df_metrics = pd.DataFrame({'Epoch': epochs, 'Actual_class': labels_flat, 'Predicted_class': pred_flat})\n        \n        tmp_eval_accuracy = accuracy_score(labels_flat, pred_flat)\n        tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n        \n        eval_accuracy += tmp_eval_accuracy\n        eval_mcc_accuracy += tmp_eval_mcc_accuracy\n        nb_eval_steps += 1\n\n    print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n    print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\nfrom tqdm.notebook import trange  # Correct import for Jupyter notebooks\n\ntrain_loss_set = []\nlearning_rate = []\n\n# Gradients get accumulated by default\nmodel.zero_grad()\n\n# Use trange from tqdm.notebook for Jupyter notebooks\nfor _ in trange(1, epochs + 1, desc='Epoch'):\n    print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n    batch_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        # Set model to training mode\n        model.train()\n        \n        # Move batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Forward pass\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs[0]\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update optimizer\n        optimizer.step()\n        \n        # Update scheduler\n        scheduler.step()\n\n        # Clear accumulated gradients\n        optimizer.zero_grad()\n        \n        batch_loss += loss.item()\n\n    # Average training loss for the epoch\n    avg_train_loss = batch_loss / len(train_dataloader)\n\n    # Store the current learning rate\n    for param_group in optimizer.param_groups:\n        print(\"\\n\\tCurrent Learning rate: \", param_group['lr'])\n        learning_rate.append(param_group['lr'])\n    \n    train_loss_set.append(avg_train_loss)\n    print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n    \n    # Validation phase\n    model.eval()\n\n    eval_accuracy, eval_mcc_accuracy, nb_eval_steps = 0, 0, 0\n\n    for batch in validation_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        logits = logits[0].to('cpu').numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        pred_flat = np.argmax(logits, axis=1).flatten()\n        labels_flat = label_ids.flatten()\n        \n        # DataFrame for metrics\n        df_metrics = pd.DataFrame({'Epoch': epochs, 'Actual_class': labels_flat, 'Predicted_class': pred_flat})\n        \n        tmp_eval_accuracy = accuracy_score(labels_flat, pred_flat)\n        tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n        \n        eval_accuracy += tmp_eval_accuracy\n        eval_mcc_accuracy += tmp_eval_mcc_accuracy\n        nb_eval_steps += 1\n\n    print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}') \n    print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:14:13.536283Z","iopub.execute_input":"2024-11-28T11:14:13.536576Z","iopub.status.idle":"2024-11-28T11:58:45.005194Z","shell.execute_reply.started":"2024-11-28T11:14:13.536543Z","shell.execute_reply":"2024-11-28T11:58:45.004272Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca4e2d36aa243cfacb1ae8ec847e9ee"}},"metadata":{}},{"name":"stdout","text":"<====================== Epoch 1 ======================>\n\n\tCurrent Learning rate:  1.3333333333333333e-05\n\n\tAverage Training loss: 0.39324043376195916\n\n\tValidation Accuracy: 0.933531746031746\n\n\tValidation MCC Accuracy: 0.9128122385092313\n<====================== Epoch 2 ======================>\n\n\tCurrent Learning rate:  6.666666666666667e-06\n\n\tAverage Training loss: 0.11971633532622145\n\n\tValidation Accuracy: 0.933531746031746\n\n\tValidation MCC Accuracy: 0.9137053098662156\n<====================== Epoch 3 ======================>\n\n\tCurrent Learning rate:  0.0\n\n\tAverage Training loss: 0.07834963053200215\n\n\tValidation Accuracy: 0.9295634920634921\n\n\tValidation MCC Accuracy: 0.9078353813516075\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2afe78e02aa5442ca0f88c4eac589e8e"}},"metadata":{}},{"name":"stdout","text":"<====================== Epoch 1 ======================>\n\n\tCurrent Learning rate:  0.0\n\n\tAverage Training loss: 0.0655934868238165\n\n\tValidation Accuracy: 0.9295634920634921\n\n\tValidation MCC Accuracy: 0.9079704430148733\n<====================== Epoch 2 ======================>\n\n\tCurrent Learning rate:  0.0\n\n\tAverage Training loss: 0.06517181211344726\n\n\tValidation Accuracy: 0.9300595238095238\n\n\tValidation MCC Accuracy: 0.9080640021634553\n<====================== Epoch 3 ======================>\n\n\tCurrent Learning rate:  0.0\n\n\tAverage Training loss: 0.06572854012039162\n\n\tValidation Accuracy: 0.9300595238095238\n\n\tValidation MCC Accuracy: 0.906976177010854\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.figure(figsize=(8, 6))  # Add figure size to make it more readable\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T12:12:36.048077Z","iopub.execute_input":"2024-11-28T12:12:36.048344Z","iopub.status.idle":"2024-11-28T12:12:36.057568Z","shell.execute_reply.started":"2024-11-28T12:12:36.048320Z","shell.execute_reply":"2024-11-28T12:12:36.056722Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"df[['label','label_desc']].drop_duplicates(keep='first') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.016904Z","iopub.execute_input":"2024-11-28T11:58:45.017096Z","iopub.status.idle":"2024-11-28T11:58:45.047018Z","shell.execute_reply.started":"2024-11-28T11:58:45.017075Z","shell.execute_reply":"2024-11-28T11:58:45.046267Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   label label_desc\n0      4    sadness\n2      0      anger\n3      3       love\n6      5   surprise\n7      1       fear\n8      2        joy","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>label_desc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>love</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>surprise</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>joy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"## emotion labels\nlabel2int = {\n  \"sadness\": 4,\n  \"joy\": 2,\n  \"anger\": 0,\n  \"fear\": 1,\n  \"surprise\": 5\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.048083Z","iopub.execute_input":"2024-11-28T11:58:45.048319Z","iopub.status.idle":"2024-11-28T11:58:45.052583Z","shell.execute_reply.started":"2024-11-28T11:58:45.048296Z","shell.execute_reply":"2024-11-28T11:58:45.051786Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":" df_metrics['Predicted_class'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.054079Z","iopub.execute_input":"2024-11-28T11:58:45.054358Z","iopub.status.idle":"2024-11-28T11:58:45.063298Z","shell.execute_reply.started":"2024-11-28T11:58:45.054326Z","shell.execute_reply":"2024-11-28T11:58:45.062559Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"array([2, 1, 0, 3, 4, 5])"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.064768Z","iopub.execute_input":"2024-11-28T11:58:45.065021Z","iopub.status.idle":"2024-11-28T11:58:45.101262Z","shell.execute_reply.started":"2024-11-28T11:58:45.064990Z","shell.execute_reply":"2024-11-28T11:58:45.098264Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-303b07786617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Actual_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel2int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel2int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1993\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m             )\n\u001b[1;32m   1997\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of classes, 6, does not match size of target_names, 5. Try specifying the labels parameter"],"ename":"ValueError","evalue":"Number of classes, 6, does not match size of target_names, 5. Try specifying the labels parameter","output_type":"error"}],"execution_count":29},{"cell_type":"markdown","source":"# Save the models for future use ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define directories for model and tokenizer\nmodel_save_folder = 'model/'\ntokenizer_save_folder = 'tokenizer/'\n\npath_model = F'/kaggle/working/{model_save_folder}'\npath_tokenizer = F'/kaggle/working/{tokenizer_save_folder}'\n\n# Create the directories for saving model and tokenizer (if they don't exist)\n!mkdir -p {path_model}\n!mkdir -p {path_tokenizer}\n\n# Save the model and tokenizer using the Hugging Face save method\nmodel.save_pretrained(path_model)\ntokenizer.save_pretrained(path_tokenizer)\n\n# Optionally save the model's state dict (this is separate from saving the model with save_pretrained)\nmodel_save_name = 'fineTuneModel.pt'\nstate_dict_path = F'{path_model}{model_save_name}'  # File path for saving model weights\ntorch.save(model.state_dict(), state_dict_path)\n\n# Confirm paths where the model and tokenizer have been saved\nprint(f\"Model saved to: {path_model}\")\nprint(f\"Tokenizer saved to: {path_tokenizer}\")  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.101927Z","iopub.status.idle":"2024-11-28T11:58:45.102194Z","shell.execute_reply":"2024-11-28T11:58:45.102058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the model and tokenizer from the saved directories\nfrom transformers import BertForSequenceClassification, BertTokenizer\n\n# Load the model\npath_model='/kaggle/working/model'\nloaded_model = BertForSequenceClassification.from_pretrained(path_model).to(device)\n\n# Load the tokenizer \npath_tokenizer='/kaggle/working/tokenizer'\nloaded_tokenizer = BertTokenizer.from_pretrained(path_tokenizer)\n\n# Confirm loading\nprint(f\"Model loaded from: {path_model}\")\nprint(f\"Tokenizer loaded from: {path_tokenizer}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.103271Z","iopub.status.idle":"2024-11-28T11:58:45.103631Z","shell.execute_reply":"2024-11-28T11:58:45.103462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# After the validation phase, where you have already obtained the predicted and true labels:\n# Store the predicted labels and actual labels\nall_predictions = []\nall_true_labels = []\n\nfor batch in validation_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    logits = logits[0].to('cpu').numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    # Get the predicted class\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    labels_flat = label_ids.flatten()\n\n    # Append predictions and true labels\n    all_predictions.extend(pred_flat)\n    all_true_labels.extend(labels_flat)\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(all_true_labels, all_predictions)\n\n# Plot the confusion matrix using seaborn heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label2int.keys(), yticklabels=label2int.keys())\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T12:15:21.517315Z","iopub.execute_input":"2024-11-28T12:15:21.517593Z","iopub.status.idle":"2024-11-28T12:15:37.012422Z","shell.execute_reply.started":"2024-11-28T12:15:21.517568Z","shell.execute_reply":"2024-11-28T12:15:37.011739Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 576x432 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":38},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.106232Z","iopub.status.idle":"2024-11-28T11:58:45.106630Z","shell.execute_reply":"2024-11-28T11:58:45.106426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForConditionalGeneration\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\nmodel = AutoModelForConditionalGeneration.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n\n# Function to predict emotion\ndef get_emotion(text):\n    input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')  # Encode the input text\n    output = model.generate(input_ids=input_ids, max_length=2)  # Generate prediction\n    dec = [tokenizer.decode(ids) for ids in output]  # Decode the output to get the emotion\n    label = dec[0]\n    return label\n\n# Test the function with examples\nprint(get_emotion(\"i feel as if i haven't blogged in ages are at least truly blogged i am doing an update cute\"))\nprint(get_emotion(\"i have a feeling i kinda lost my best friend\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T11:58:45.108524Z","iopub.status.idle":"2024-11-28T11:58:45.108917Z","shell.execute_reply":"2024-11-28T11:58:45.108725Z"}},"outputs":[],"execution_count":null}]}
